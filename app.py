import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import warnings
import shap

# --- 0. CONFIGURATION ---
st.set_page_config(
    page_title="Trustpilot Sentiment IA",
    page_icon="‚≠ê",
    layout="wide"
)
warnings.filterwarnings("ignore", category=UserWarning)

# --- 1. CHARGEMENT DES RESSOURCES (CACHE) ---
@st.cache_resource
def download_nltk_resources():
    resources = ['punkt', 'stopwords', 'wordnet', 'punkt_tab']
    for res in resources:
        try:
            nltk.data.find(f'tokenizers/{res}')
        except LookupError:
            nltk.download(res, quiet=True)

download_nltk_resources()

@st.cache_resource
def load_model_assets():
    try:
        model = joblib.load('trustpilot_lgbm_model.pkl')
        vectorizer = joblib.load('tfidf_vectorizer.pkl')
        return model, vectorizer
    except FileNotFoundError:
        return None, None

model, vectorizer = load_model_assets()

# --- 2. PIPELINE DE NETTOYAGE ---
stop_words = set(stopwords.words('english'))
stop_words.update([",", ".", "``", "@", "*", "(", ")", "[","]", "...", "-", "_", ">", "<", ":", "/", "//", "///", "=", "--", "¬©", "~", ";", "\\", "\\\\", '"', "'","''", '""' "'m", "'ve", "n't","!","?", "'re", "rd", "'s", "%"])
lemmatizer = WordNetLemmatizer()

def processing_pipeline(text):
    if not isinstance(text, str): return ""
    text = text.lower()
    text = re.sub(r"\.+", '', text)
    text = re.sub(r"/", ' ', text)
    text = re.sub(r"[0-9]+", '', text)
    try:
        tokens = word_tokenize(text, language='english')
    except:
        tokens = text.split()
    
    cleaned_tokens = []
    for token in tokens:
        if token not in stop_words:
            lemma = lemmatizer.lemmatize(token)
            cleaned_tokens.append(lemma)
    return " ".join(cleaned_tokens)

# --- 3. SIDEBAR ---
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/commons/a/a9/Amazon_logo.svg", width=150)
    st.header("üîç Infos du Mod√®le")
    st.info("Mod√®le : LightGBM")
    st.write("Vectorisation : TF-IDF")
    st.metric(label="Pr√©cision (Accuracy)", value="71.8%", delta="+vs Baseline")
    st.markdown("---")
    st.caption("Projet DataScientest\nLakdar & Aurore")

# --- 4. TITRE PRINCIPAL ---
st.title("üõçÔ∏è Analyse de Sentiment & Exp√©rience Client")
st.markdown("Application de d√©monstration pour la pr√©diction de satisfaction √† partir d'avis textuels.")

# --- 5. CR√âATION DES ONGLETS ---
tab_demo, tab_data, tab_model = st.tabs(["üöÄ D√©mo Live", "üìä Jeu de Donn√©es", "ü§ñ Performance Mod√®le"])

# ==============================================================================
# ONGLET 1 : D√âMO LIVE (AVEC GRAPHIQUE ROBUSTE)
# ==============================================================================
with tab_demo:
    if model is None:
        st.error("‚ö†Ô∏è Erreur : Fichiers .pkl introuvables.")
    else:
        if "text_input" not in st.session_state:
            st.session_state.text_input = ""

        def set_text(text):
            st.session_state.text_input = text

        st.subheader("Testez l'IA en temps r√©el")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.button("üò° N√©gatif", on_click=set_text, args=["Horrible service, I waited 2 weeks and the package is broken. Never again!"], use_container_width=True)
        with col2:
            st.button("üòê Neutre", on_click=set_text, args=["The product is okay but shipping was a bit slow. Not bad, not great."], use_container_width=True)
        with col3:
            st.button("üòç Positif", on_click=set_text, args=["Absolutely amazing! Best purchase of the year, highly recommended."], use_container_width=True)

        user_input = st.text_area("Votre commentaire :", value=st.session_state.text_input, height=100)

        # --- PREDICTION ---
        if st.button("Lancer l'analyse", type="primary"):
            if user_input.strip():
                with st.spinner('Analyse et identification des mots-cl√©s...'):
                    # 1. Pipeline
                    clean_text = processing_pipeline(user_input)
                    vec_input = vectorizer.transform([clean_text])
                    input_array = vec_input.toarray()
                    
                    pred_class = model.predict(input_array)[0]
                    pred_proba = model.predict_proba(input_array)[0]
                    
                    labels = {0: ("N√©gatif üòû", "red"), 1: ("Neutre üòê", "orange"), 2: ("Positif üòÉ", "green")}
                    label_text, color = labels[pred_class]
                    confidence = pred_proba[pred_class]

                    # 2. R√©sultats
                    st.divider()
                    c1, c2 = st.columns([1, 2])
                    
                    with c1:
                        st.markdown("### Verdict :")
                        st.markdown(f":{color}[**{label_text}**]")
                        st.metric("Confiance", f"{confidence:.1%}")
                    
                    with c2:
                        st.markdown("#### Probabilit√©s")
                        df_chart = pd.DataFrame({"Sentiment": ["N√©gatif", "Neutre", "Positif"], "Probabilit√©": pred_proba})
                        c = alt.Chart(df_chart).mark_bar().encode(
                            x=alt.X('Sentiment', sort=None),
                            y='Probabilit√©',
                            color=alt.Color('Sentiment', scale=alt.Scale(domain=["N√©gatif", "Neutre", "Positif"], range=["#6D6D6D", "#FFB7B2", "#FF69B4"]), legend=None)
                        )
                        st.altair_chart(c, use_container_width=True)
                    
                    # 3. INTERPR√âTABILIT√â (M√âTHODE ROBUSTE SANS SHAP)
                    st.markdown("---")
                    st.subheader("üß† Analyse des mots cl√©s")
                    st.write(f"Voici les termes d√©tect√©s qui ont le plus de poids dans la d√©cision :")

                    try:
                        # On r√©cup√®re l'importance globale des features du mod√®le
                        feature_importances = model.feature_importances_
                        feature_names = vectorizer.get_feature_names_out()
                        
                        # On regarde quels mots sont pr√©sents dans la phrase de l'utilisateur
                        # On multiplie la pr√©sence du mot (TF-IDF) par son importance globale
                        indices = input_array[0].nonzero()[0]
                        
                        if len(indices) > 0:
                            words_found = []
                            scores = []
                            for idx in indices:
                                words_found.append(feature_names[idx])
                                # Score = Poids global du mot * sa pr√©sence TF-IDF
                                scores.append(feature_importances[idx] * input_array[0][idx])
                            
                            # Cr√©ation du DataFrame pour le graph
                            df_impact = pd.DataFrame({"Mot": words_found, "Impact": scores})
                            df_impact = df_impact.sort_values(by="Impact", ascending=False).head(10) # Top 10
                            
                            # Couleur des barres selon le sentiment pr√©dit
                            bar_color = "#FF4B4B" if pred_class == 0 else "#FFA500" if pred_class == 1 else "#2E8B57"

                            chart_impact = alt.Chart(df_impact).mark_bar(color=bar_color).encode(
                                x=alt.X('Impact', title='Poids dans la d√©cision'),
                                y=alt.Y('Mot', sort='-x', title='Mots d√©tect√©s'),
                                tooltip=['Mot', 'Impact']
                            )
                            st.altair_chart(chart_impact, use_container_width=True)
                            st.caption("Ces mots ont √©t√© reconnus par le mod√®le comme √©tant d√©terminants.")
                        else:
                            st.info("Aucun mot-cl√© significatif d√©tect√© dans le vocabulaire du mod√®le.")

                    except Exception as e:
                        st.warning(f"D√©tail indisponible (Pas assez de donn√©es).")

                    with st.expander("üëÄ Voir le texte nettoy√©"):
                        st.code(clean_text)
            else:
                st.warning("Veuillez entrer du texte.")

        # --- CSV BULK ---
        st.markdown("---")
        st.subheader("üìÇ Analyse de masse (Fichier CSV)")
        csv_template = "text\nExemple: Super produit !\nExemple: Livraison trop longue..."
        st.download_button("üì• T√©l√©charger mod√®le CSV", csv_template, "modele_avis.csv", "text/csv")
        
        uploaded_file = st.file_uploader("D√©posez votre fichier ici", type=["csv"])
        if uploaded_file is not None:
            try:
                df = pd.read_csv(uploaded_file)
                cols = [c for c in df.columns if 'text' in c.lower() or 'review' in c.lower()]
                if cols:
                    if st.button(f"Analyser {len(df)} avis"):
                        target_col = cols[0]
                        df['clean'] = df[target_col].astype(str).apply(processing_pipeline)
                        vecs = vectorizer.transform(df['clean'])
                        preds = model.predict(vecs.toarray())
                        mapping = {0: "N√©gatif", 1: "Neutre", 2: "Positif"}
                        df['Prediction'] = [mapping[p] for p in preds]
                        
                        st.dataframe(df[[target_col, 'Prediction']], use_container_width=True)
                        st.download_button("üì• T√©l√©charger r√©sultats", df.to_csv(index=False).encode('utf-8'), "resultats.csv", "text/csv")
            except Exception as e:
                st.error(f"Erreur CSV : {e}")

# ==============================================================================
# ONGLET 2 : JEU DE DONN√âES (Pr√©sentation)
# ==============================================================================
with tab_data:
    st.header("üìö Le Jeu de Donn√©es : Amazon Electronics")
    
    col_d1, col_d2 = st.columns([1, 2])
    with col_d1:
        st.markdown("### Source & Pourquoi ?")
        st.image("https://upload.wikimedia.org/wikipedia/commons/a/a9/Amazon_logo.svg", width=100)
        st.info("**Amazon Reviews Dataset**")
        st.write("- **Structure identique** : Texte + Note")
        st.write("- **Focus Electronics** : Vocabulaire riche")
        st.write("- **P√©riode** : 2010 - 2018")
    
    with col_d2:
        st.markdown("### Volum√©trie & Nettoyage")
        metrics_df = pd.DataFrame({"M√©trique": ["Avis bruts", "Avis apr√®s filtrage", "Langue"], "Valeur": ["~1.2 Millions", "572 950", "Anglais"]})
        st.dataframe(metrics_df, hide_index=True, use_container_width=True)
        
        c1, c2, c3 = st.columns(3)
        c1.error("‚ùå **Prix**\n\nSupprim√© (trop de NAs)")
        c2.warning("‚ö†Ô∏è **Votes**\n\nImput√© √† 0 (NAs)")
        c3.info("üñºÔ∏è **Image**\n\nBool√©en (Y/N)")

    st.divider()
    st.subheader("üìã Aper√ßu des donn√©es brutes (Exemple)")
    example_data = {
        "overall": [5, 1, 3, 5, 2],
        "summary": ["Amazing sound", "Waste of money", "Average", "Great service", "Disappointed"],
        "reviewText": ["This headphone is amazing! The bass is deep.", "Terrible quality, stopped working.", "It's okay for the price.", "Works perfectly, fast delivery.", "Poor screen resolution."],
        "brand": ["Bose", "Generic", "Sony", "Samsung", "LG"]
    }
    st.dataframe(pd.DataFrame(example_data), use_container_width=True)

    st.markdown("---")
    st.subheader("Distribution des classes (√âquilibr√©e)")
    chart_balance = alt.Chart(pd.DataFrame({"Sentiment": ["N√©gatif", "Neutre", "Positif"], "Nombre": [190983, 190983, 190983]})).mark_bar().encode(
        x=alt.X('Sentiment', sort=None), y='Nombre',
        color=alt.Color('Sentiment', scale=alt.Scale(range=["#6D6D6D", "#FFB7B2", "#FF69B4"]), legend=None)
    )
    st.altair_chart(chart_balance, use_container_width=True)

# ==============================================================================
# ONGLET 3 : PERFORMANCE MOD√àLE
# ==============================================================================
with tab_model:
    st.header("‚öôÔ∏è Performance LightGBM")
    m1, m2, m3 = st.columns(3)
    m1.metric("Accuracy Globale", "71.8%")
    m2.metric("F1-Score", "0.72")
    m3.metric("Vocabulaire", "5 000 mots")

    st.subheader("Matrice de Confusion")
    # Donn√©es issues de ton rapport
    confusion_data = pd.DataFrame(
        [[8303, 2155, 558], [2303, 6734, 1979], [551, 1765, 8700]],
        columns=["Pr√©dit N√©gatif", "Pr√©dit Neutre", "Pr√©dit Positif"],
        index=["R√©el N√©gatif", "R√©el Neutre", "R√©el Positif"]
    )
    st.dataframe(confusion_data, use_container_width=True)
    
    st.success("‚úÖ **Observation :** Tr√®s bonne d√©tection des avis positifs et n√©gatifs.")
    st.warning("‚ö†Ô∏è **Limite :** La classe 'Neutre' (au centre) est celle qui g√©n√®re le plus de confusion.")
    
    st.markdown("---")
    st.subheader("Global Feature Importance")
    st.write("Mots les plus impactants pour le mod√®le (Global) :")
    col_feat1, col_feat2 = st.columns(2)
    with col_feat1:
        st.error("üìâ **N√©gatif** : bad, poor, waste, return, money")
    with col_feat2:
        st.success("üìà **Positif** : great, love, good, easy, perfect")

