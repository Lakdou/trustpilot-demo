import streamlit as st
import pandas as pd
import numpy as np
import joblib
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# --- CONFIGURATION DE LA PAGE ---
st.set_page_config(
    page_title="Trustpilot Sentiment IA",
    page_icon="â­",
    layout="centered"
)

# --- CHARGEMENT DES RESSOURCES NLTK (CACHE) ---
# NÃ©cessaire pour que Ã§a marche sur n'importe quel ordi ou sur le Cloud
@st.cache_resource
def download_nltk_resources():
    resources = ['punkt', 'stopwords', 'wordnet', 'punkt_tab']
    for res in resources:
        try:
            nltk.data.find(f'tokenizers/{res}')
        except LookupError:
            nltk.download(res, quiet=True)
    try:
        nltk.data.find('corpora/wordnet')
    except LookupError:
        nltk.download('wordnet', quiet=True)

download_nltk_resources()

# --- CHARGEMENT DU MODÃˆLE ET VECTORISEUR ---
@st.cache_resource
def load_model_assets():
    try:
        model = joblib.load('trustpilot_lgbm_model.pkl')
        vectorizer = joblib.load('tfidf_vectorizer.pkl')
        return model, vectorizer
    except FileNotFoundError:
        return None, None

model, vectorizer = load_model_assets()

# --- FONCTION DE NETTOYAGE (Identique Ã  ton entraÃ®nement) ---
# On initialise les outils une seule fois
stop_words = set(stopwords.words('english'))
# Ta liste personnalisÃ©e de stopwords/ponctuation
stop_words.update([",", ".", "``", "@", "*", "(", ")", "[","]", "...", "-", "_", ">", "<", ":", "/", "//", "///", "=", "--", "Â©", "~", ";", "\\", "\\\\", '"', "'","''", '""' "'m", "'ve", "n't","!","?", "'re", "rd", "'s", "%"])
lemmatizer = WordNetLemmatizer()

def processing_pipeline(text):
    if not isinstance(text, str): return ""
    
    # 1. Lowercase
    text = text.lower()
    
    # 2. Regex (Tes rÃ¨gles exactes)
    text = re.sub(r"\.+", '', text)      # Points multiples
    text = re.sub(r"/", ' ', text)       # Slashes
    text = re.sub(r"[0-9]+", '', text)   # Chiffres
    
    # 3. Tokenisation
    try:
        tokens = word_tokenize(text, language='english')
    except:
        # Fallback simple si punkt_tab plante
        tokens = text.split()
    
    # 4. Stopwords & Lemmatisation
    cleaned_tokens = []
    seen = set() # Pour Ã©viter les doublons dans la mÃªme phrase si tu veux
    
    for token in tokens:
        if token not in stop_words:
            lemma = lemmatizer.lemmatize(token)
            cleaned_tokens.append(lemma)
            
    # 5. Rejoin (TF-IDF a besoin d'une string, pas d'une liste)
    return " ".join(cleaned_tokens)

# --- INTERFACE UTILISATEUR ---

st.title("ðŸ›ï¸ Analyse d'Avis Trustpilot")
st.markdown("""
Cette IA analyse le texte d'un commentaire pour prÃ©dire si l'expÃ©rience client a Ã©tÃ© :
**NÃ©gative** (1-2â­), **Neutre** (3â­) ou **Positive** (4-5â­).
""")

if model is None:
    st.error("âš ï¸ Erreur : Les fichiers `.pkl` sont introuvables. VÃ©rifiez qu'ils sont bien dans le mÃªme dossier que app.py")
else:
    # Zone de saisie
    user_input = st.text_area("Copiez un avis client ici (en anglais) :", height=100, placeholder="Example: The delivery was very fast but the product quality is poor...")

    if st.button("Lancer l'analyse", type="primary"):
        if user_input.strip():
            with st.spinner('Nettoyage et analyse en cours...'):
                
                # 1. PrÃ©traitement
                clean_text = processing_pipeline(user_input)
                
                # 2. Vectorisation
                vec_input = vectorizer.transform([clean_text])
                
                # 3. PrÃ©diction
                pred_class = model.predict(vec_input)[0]
                pred_proba = model.predict_proba(vec_input)[0]
                
                # Mapping des classes (0, 1, 2) vers Labels
                # Rappel: Tu as fait y_train - 1, donc : 0=Neg, 1=Neu, 2=Pos
                labels = {
                    0: ("NÃ©gatif ðŸ˜ž", "red"),
                    1: ("Neutre ðŸ˜", "orange"),
                    2: ("Positif ðŸ˜ƒ", "green")
                }
                
                label_text, color = labels[pred_class]
                confidence = pred_proba[pred_class]

                # --- AFFICHAGE DES RÃ‰SULTATS ---
                st.divider()
                
                col1, col2 = st.columns([1, 2])
                
                with col1:
                    st.markdown(f"### Verdict :")
                    st.markdown(f":{color}[**{label_text}**]")
                    st.metric("Niveau de confiance", f"{confidence:.1%}")
                
                with col2:
                    st.markdown("#### ProbabilitÃ©s par classe")
                    # CrÃ©ation d'un petit dataframe pour le graph
                    chart_data = pd.DataFrame(
                        pred_proba.reshape(1, 3), 
                        columns=["NÃ©gatif", "Neutre", "Positif"]
                    )
                    st.bar_chart(chart_data.T)

                # Section Debug (Toujours sympa pour la dÃ©mo)
                with st.expander("ðŸ‘€ Voir ce que l'IA a 'lu' (Texte nettoyÃ©)"):
                    st.write(f"**Brut :** {user_input}")
                    st.write(f"**NettoyÃ© & LemmatisÃ© :** {clean_text}")

        else:
            st.warning("Veuillez entrer du texte pour analyser.")

# Footer
st.markdown("---")
st.caption("Projet Ã‰cole - ModÃ¨le LightGBM + TF-IDF")